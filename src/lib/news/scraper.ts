/**
 * M√≥dulo de Scraping de Noticias
 *
 * Arquitectura H√≠brida:
 * - FASE 1 (Firecrawl): Descubrimiento r√°pido de URLs en p√°ginas de categor√≠as
 * - FASE 2 (Crawl4AI): Extracci√≥n completa de art√≠culos con contenido full
 */

import {
  getActiveSources,
  updateSourceLastScraped,
} from "@/lib/db/queries/sources";
import type { NewsSource } from "@/lib/schema";
import { extractArticles } from "@/lib/crawl4ai";

/**
 * Constantes de configuraci√≥n
 */
const SCRAPE_TIMEOUT = 120000; // 120 segundos (2 minutos) - Firecrawl puede ser lento
const MAX_RETRIES = 2; // Reducir a 2 intentos ya que cada uno toma m√°s tiempo
const RETRY_DELAYS = [3000, 5000]; // 3s, 5s

/**
 * Obtiene la URL del API de Firecrawl desde variables de entorno
 */
function getFirecrawlApiUrl(): string {
  const baseUrl = process.env.FIRECRAWL_API_URL || "https://api.firecrawl.dev";
  // Asegurar que termina con /v1/scrape
  return baseUrl.endsWith("/v1/scrape")
    ? baseUrl
    : `${baseUrl}/v1/scrape`;
}

/**
 * Estructura de un art√≠culo scrapeado
 */
export interface ScrapedArticle {
  id?: string; // UUID generado para identificar la noticia
  title: string;
  content: string; // Excerpt/resumen del art√≠culo (de Firecrawl)
  fullContent?: string; // Contenido completo del art√≠culo (de Crawl4AI)
  url: string;
  imageUrl?: string; // URL de la imagen principal
  author?: string; // Autor del art√≠culo (de Crawl4AI)
  publishedDate?: string; // Fecha de publicaci√≥n (de Crawl4AI)
  source: string;
  selected?: boolean; // Si est√° seleccionada para el bolet√≠n (por defecto true)
  scrapedAt?: string; // Timestamp del scraping
  metadata?: {
    wordCount?: number; // N√∫mero de palabras en el art√≠culo
    readingTime?: number; // Tiempo estimado de lectura (minutos)
    contentQuality?: number; // Score de calidad del contenido (0-100)
  };
}

/**
 * Resultado del scraping
 */
export interface ScrapeResult {
  primicias: ScrapedArticle[];
  laHora: ScrapedArticle[];
  elComercio: ScrapedArticle[];
  teleamazonas: ScrapedArticle[];
  ecu911: ScrapedArticle[];
  metadata: {
    totalArticles: number;
    scrapedAt: string;
    sourcesSuccess: number;
    sourcesFailed: number;
    errors: Array<{ source: string; error: string }>;
  };
}

/**
 * Configuraci√≥n de scraping de una fuente
 */
interface ScrapeConfig {
  urls?: string[];
  onlyMainContent?: boolean;
  waitFor?: number;
  removeBase64Images?: boolean;
}

/**
 * Estructura de respuesta de Firecrawl
 */
interface FirecrawlResponse {
  success: boolean;
  data?: {
    markdown?: string;
    html?: string;
    metadata?: Record<string, unknown>;
  };
  error?: string;
}

/**
 * Espera un tiempo espec√≠fico (helper para delays)
 */
function sleep(ms: number): Promise<void> {
  return new Promise((resolve) => setTimeout(resolve, ms));
}

/**
 * Normaliza el nombre de la fuente para usar como key
 *
 * @param name - Nombre de la fuente
 * @returns Key normalizada (camelCase)
 *
 * @example
 * ```ts
 * normalizeSourceName("Primicias") // "primicias"
 * normalizeSourceName("La Hora") // "laHora"
 * normalizeSourceName("El Comercio") // "elComercio"
 * ```
 */
function normalizeSourceName(name: string): keyof Omit<ScrapeResult, "metadata"> {
  const normalized = name
    .toLowerCase()
    .replace(/\s+/g, "")
    .replace(/^el/, "el")
    .replace(/^la/, "la");

  // Mapeo espec√≠fico
  const mapping: Record<string, keyof Omit<ScrapeResult, "metadata">> = {
    primicias: "primicias",
    lahora: "laHora",
    elcomercio: "elComercio",
    teleamazonas: "teleamazonas",
    ecu911: "ecu911",
  };

  return mapping[normalized] || "primicias";
}

/**
 * Scrapea todas las fuentes activas
 *
 * @returns Resultado del scraping con art√≠culos organizados por fuente
 *
 * @example
 * ```ts
 * const result = await scrapeAllSources();
 * console.log(`Total: ${result.metadata.totalArticles} art√≠culos`);
 * console.log(`Primicias: ${result.primicias.length} art√≠culos`);
 * ```
 */
export async function scrapeAllSources(): Promise<ScrapeResult> {
  console.log("üîç Iniciando scraping de todas las fuentes...");

  const result: ScrapeResult = {
    primicias: [],
    laHora: [],
    elComercio: [],
    teleamazonas: [],
    ecu911: [],
    metadata: {
      totalArticles: 0,
      scrapedAt: new Date().toISOString(),
      sourcesSuccess: 0,
      sourcesFailed: 0,
      errors: [],
    },
  };

  try {
    // Obtener fuentes activas
    const sources = await getActiveSources();
    console.log(`üì° Encontradas ${sources.length} fuentes activas`);

    // Scrapear cada fuente
    const scrapePromises = sources.map(async (source) => {
      try {
        console.log(`  ‚Üí Scraping ${source.name}...`);
        const articles = await scrapeSource(source);

        // Actualizar fuente como exitosa
        await updateSourceLastScraped(source.id, "success");

        // Agregar art√≠culos al resultado
        const sourceKey = normalizeSourceName(source.name);
        result[sourceKey] = articles;
        result.metadata.sourcesSuccess++;

        console.log(`  ‚úì ${source.name}: ${articles.length} art√≠culos`);
      } catch (error) {
        console.error(`  ‚úó ${source.name} fall√≥:`, error);

        // Actualizar fuente como fallida
        await updateSourceLastScraped(source.id, "failed");

        result.metadata.sourcesFailed++;
        result.metadata.errors.push({
          source: source.name,
          error: (error as Error).message,
        });
      }
    });

    // Esperar a que todas las fuentes terminen
    await Promise.all(scrapePromises);

    // Calcular total de art√≠culos
    result.metadata.totalArticles =
      result.primicias.length +
      result.laHora.length +
      result.elComercio.length +
      result.teleamazonas.length +
      result.ecu911.length;

    console.log(`‚úÖ Scraping completado: ${result.metadata.totalArticles} art√≠culos totales`);

    return result;
  } catch (error) {
    console.error("‚ùå Error en scrapeAllSources:", error);
    throw new Error(
      `Error scraping fuentes: ${(error as Error).message}`
    );
  }
}

/**
 * Enriquece art√≠culos con contenido completo usando Crawl4AI
 *
 * FASE 2 del pipeline h√≠brido: Despu√©s de descubrir URLs con Firecrawl,
 * esta funci√≥n extrae el contenido completo de cada art√≠culo.
 *
 * @param result - Resultado del scraping inicial (FASE 1)
 * @param options - Opciones de configuraci√≥n
 * @returns Resultado enriquecido con contenido completo
 *
 * @example
 * ```ts
 * const basicResult = await scrapeAllSources();
 * const enrichedResult = await enrichWithFullContent(basicResult);
 * console.log(`Enriquecidos: ${enrichedResult.metadata.totalArticles} art√≠culos`);
 * ```
 */
export async function enrichWithFullContent(
  result: ScrapeResult,
  options?: {
    maxConcurrency?: number;
    enableCrawl4AI?: boolean;
  }
): Promise<ScrapeResult> {
  const enableCrawl4AI = options?.enableCrawl4AI ?? true;

  if (!enableCrawl4AI) {
    console.log("‚è≠Ô∏è  Crawl4AI deshabilitado, saltando enriquecimiento");
    return result;
  }

  console.log("üöÄ Iniciando FASE 2: Enriquecimiento con Crawl4AI...");

  const enrichedResult: ScrapeResult = { ...result };
  let totalEnriched = 0;
  let totalFailed = 0;

  try {
    // Procesar cada fuente
    const sources: Array<keyof Omit<ScrapeResult, "metadata">> = [
      "primicias",
      "laHora",
      "elComercio",
      "teleamazonas",
      "ecu911",
    ];

    for (const sourceKey of sources) {
      const articles = result[sourceKey];

      if (articles.length === 0) {
        console.log(`  ‚è≠Ô∏è  ${sourceKey}: Sin art√≠culos para enriquecer`);
        continue;
      }

      console.log(`  üîç ${sourceKey}: Enriqueciendo ${articles.length} art√≠culos...`);

      try {
        // Extraer URLs de los art√≠culos
        const urls = articles.map((article) => article.url);
        const sourceName = articles[0].source;

        // Usar Crawl4AI para extraer contenido completo
        const fullArticles = await extractArticles(
          urls,
          sourceName,
          options?.maxConcurrency || 5
        );

        // Crear mapa de art√≠culos completos por URL
        const fullArticleMap = new Map(fullArticles.map((article) => [article.url, article]));

        // Merge: Combinar datos de Firecrawl (excerpts) con Crawl4AI (contenido completo)
        const mergedArticles = articles.map((article) => {
          const fullArticle = fullArticleMap.get(article.url);

          if (fullArticle) {
            totalEnriched++;
            return {
              ...article,
              fullContent: fullArticle.fullContent,
              author: fullArticle.author || article.author,
              publishedDate: fullArticle.publishedDate || article.publishedDate,
              imageUrl: fullArticle.imageUrl || article.imageUrl,
              metadata: fullArticle.metadata,
            };
          } else {
            totalFailed++;
            console.warn(`  ‚ö†Ô∏è  ${sourceKey}: No se pudo enriquecer ${article.url}`);
            return article;
          }
        });

        enrichedResult[sourceKey] = mergedArticles;

        console.log(
          `  ‚úì ${sourceKey}: ${fullArticles.length}/${articles.length} art√≠culos enriquecidos`
        );
      } catch (error) {
        console.error(`  ‚úó ${sourceKey} fall√≥ en enriquecimiento:`, error);
        // Mantener art√≠culos originales si falla
        enrichedResult[sourceKey] = articles;
        totalFailed += articles.length;
      }
    }

    console.log(
      `‚úÖ Enriquecimiento completado: ${totalEnriched} exitosos, ${totalFailed} fallidos`
    );

    return enrichedResult;
  } catch (error) {
    console.error("‚ùå Error en enrichWithFullContent:", error);
    // En caso de error, retornar resultado original sin enriquecer
    return result;
  }
}

/**
 * Scrapea una fuente individual usando Firecrawl API
 *
 * @param source - Fuente a scrapear
 * @returns Array de art√≠culos scrapeados
 *
 * @throws Error si el scraping falla despu√©s de todos los reintentos
 *
 * @example
 * ```ts
 * const source = await getSourceByName("Primicias");
 * const articles = await scrapeSource(source);
 * ```
 */
export async function scrapeSource(source: NewsSource): Promise<ScrapedArticle[]> {
  const apiKey = process.env.FIRECRAWL_API_KEY;

  if (!apiKey) {
    throw new Error("FIRECRAWL_API_KEY no est√° configurada");
  }

  // Verificar si hay m√∫ltiples URLs en scrapeConfig
  const scrapeConfig = source.scrapeConfig as ScrapeConfig | null;
  const urls = scrapeConfig?.urls || [source.url];

  console.log(`  üìã ${source.name}: ${urls.length} URL(s) a scrapear`);

  // Scrapear cada URL
  const allArticles: ScrapedArticle[] = [];

  for (let urlIndex = 0; urlIndex < urls.length; urlIndex++) {
    const url = urls[urlIndex];
    const urlLabel = urls.length > 1 ? `${urlIndex + 1}/${urls.length}` : "";

    console.log(`  üîó ${source.name} ${urlLabel}: ${url}`);

    try {
      const articles = await scrapeURL(url, source, apiKey);
      allArticles.push(...articles);
      console.log(`  ‚úì ${source.name} ${urlLabel}: ${articles.length} art√≠culos`);
    } catch (error) {
      console.error(`  ‚ö†Ô∏è  ${source.name} ${urlLabel} fall√≥:`, (error as Error).message);
      // Continuar con la siguiente URL aunque esta falle
    }
  }

  console.log(`  ‚úÖ ${source.name}: Total ${allArticles.length} art√≠culos de ${urls.length} URL(s)`);

  return allArticles;
}

/**
 * Scrapea una URL espec√≠fica usando Firecrawl API
 *
 * @param url - URL a scrapear
 * @param source - Fuente de la cual proviene la URL
 * @param apiKey - API key de Firecrawl
 * @returns Array de art√≠culos scrapeados
 */
async function scrapeURL(
  url: string,
  source: NewsSource,
  apiKey: string
): Promise<ScrapedArticle[]> {
  let lastError: Error | null = null;

  // Intentar scraping con reintentos
  for (let attempt = 0; attempt < MAX_RETRIES; attempt++) {
    const attemptStart = Date.now();
    console.log(`  üîÑ Intento ${attempt + 1}/${MAX_RETRIES} para ${source.name}...`);

    try {
      const controller = new AbortController();
      const timeoutId = setTimeout(() => {
        console.log(`  ‚è∞ Timeout alcanzado para ${source.name} (${SCRAPE_TIMEOUT / 1000}s)`);
        controller.abort();
      }, SCRAPE_TIMEOUT);

      const apiUrl = getFirecrawlApiUrl();
      if (attempt === 0) {
        console.log(`  üåê Using Firecrawl API: ${apiUrl}`);
      }

      // POST a Firecrawl API
      const fetchStart = Date.now();
      const response = await fetch(apiUrl, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          Authorization: `Bearer ${apiKey}`,
        },
        body: JSON.stringify({
          url: url, // Usar el par√°metro url, no source.url
          formats: ["markdown", "html"],
          onlyMainContent: true,
          waitFor: 0, // No esperar, scrapear inmediatamente
          removeBase64Images: true,
          timeout: 90000, // 90 segundos de timeout en Firecrawl
          mobile: false, // Desktop user agent (m√°s r√°pido)
          skipTlsVerification: false,
        }),
        signal: controller.signal,
      });

      clearTimeout(timeoutId);

      const fetchDuration = Date.now() - fetchStart;
      console.log(`  ‚è±Ô∏è  Fetch completado en ${(fetchDuration / 1000).toFixed(2)}s`);

      // Verificar respuesta
      if (!response.ok) {
        const errorText = await response.text();
        console.error(`  ‚ùå Error HTTP ${response.status} de Firecrawl:`, errorText.substring(0, 200));
        throw new Error(`HTTP ${response.status}: ${response.statusText}`);
      }

      const data: FirecrawlResponse = await response.json();

      if (!data.success || !data.data) {
        console.error(`  ‚ùå Firecrawl retorn√≥ error:`, data.error || "Sin datos");
        throw new Error(data.error || "Firecrawl no retorn√≥ datos");
      }

      console.log(`  ‚úì Respuesta de Firecrawl recibida exitosamente`);

      // Parsear art√≠culos usando la nueva funci√≥n de categor√≠as
      const articles = parseCategoryPage(data, source.name, url);

      const totalDuration = Date.now() - attemptStart;
      console.log(`  ‚úÖ ${source.name} completado en ${(totalDuration / 1000).toFixed(2)}s`);

      return articles;
    } catch (error) {
      lastError = error as Error;
      console.error(
        `  Intento ${attempt + 1}/${MAX_RETRIES} fall√≥ para ${source.name}:`,
        (error as Error).message
      );

      // Si no es el √∫ltimo intento, esperar antes de reintentar
      if (attempt < MAX_RETRIES - 1) {
        await sleep(RETRY_DELAYS[attempt]);
      }
    }
  }

  // Si llegamos aqu√≠, todos los intentos fallaron
  throw new Error(
    `Scraping de ${source.name} fall√≥ despu√©s de ${MAX_RETRIES} intentos: ${lastError?.message}`
  );
}

/**
 * Parsea una p√°gina de categor√≠a y extrae m√∫ltiples noticias
 * Esta funci√≥n detecta "cards" de noticias en p√°ginas de categor√≠as
 *
 * @param response - Respuesta de Firecrawl API
 * @param sourceName - Nombre de la fuente
 * @param sourceUrl - URL de la p√°gina scrapeada
 * @returns Array de art√≠culos extra√≠dos
 */
export function parseCategoryPage(
  response: FirecrawlResponse,
  sourceName: string,
  sourceUrl: string
): ScrapedArticle[] {
  if (!response.data) {
    console.log(`  ‚ö†Ô∏è  ${sourceName}: No data in response`);
    return [];
  }

  const articles: ScrapedArticle[] = [];
  const markdown = response.data.markdown || "";
  const html = response.data.html || "";

  console.log(`  üìä ${sourceName}: Parseando p√°gina de categor√≠a...`);
  console.log(`  üìÑ markdown=${markdown.length} chars, html=${html.length} chars`);

  try {
    // ESTRATEGIA 1: Buscar enlaces en markdown con t√≠tulos
    // Patr√≥n: [T√≠tulo](URL)
    const markdownLinks = markdown.matchAll(/\[([^\]]{20,200})\]\((https?:\/\/[^\)]+)\)/g);
    const linksFound = Array.from(markdownLinks);

    console.log(`  üîç Estrategia 1 (markdown links): ${linksFound.length} enlaces encontrados`);

    for (const match of linksFound.slice(0, 5)) { // M√°ximo 5 noticias
      const title = match[1].trim();
      const url = match[2];

      // Buscar contenido cercano al link
      const linkIndex = markdown.indexOf(match[0]);
      const contentAfter = markdown.substring(linkIndex + match[0].length, linkIndex + 500);
      const content = contentAfter
        .replace(/\[([^\]]+)\]\([^\)]+\)/g, '') // Remover otros links
        .replace(/[#*_]/g, '') // Remover markdown syntax
        .trim()
        .substring(0, 300);

      // Buscar imagen en markdown cercana
      const imageMatch = markdown.substring(linkIndex - 200, linkIndex + 200)
        .match(/!\[([^\]]*)\]\((https?:\/\/[^\)]+)\)/);
      const imageUrl = imageMatch ? imageMatch[2] : undefined;

      const article: ScrapedArticle = {
        id: crypto.randomUUID(),
        title,
        content: content || title, // Si no hay contenido, usar el t√≠tulo
        url,
        imageUrl,
        source: sourceName,
        selected: true,
        scrapedAt: new Date().toISOString(),
      };

      if (validateCategoryArticle(article)) {
        articles.push(article);
      }
    }

    // ESTRATEGIA 2: Si no encontramos suficientes, buscar en HTML
    if (articles.length < 3 && html) {
      console.log(`  üîç Estrategia 2 (HTML parsing): Buscando noticias en HTML...`);

      // Buscar enlaces que parezcan noticias (con texto largo)
      const linkPattern = /<a[^>]+href=["']([^"']+)["'][^>]*>([^<]{20,200})<\/a>/gi;
      const htmlLinks = Array.from(html.matchAll(linkPattern));

      console.log(`  üì∞ Encontrados ${htmlLinks.length} enlaces en HTML`);

      for (const match of htmlLinks.slice(0, 5)) {
        const url = match[1];
        const title = match[2].replace(/<[^>]+>/g, '').trim();

        // Filtrar URLs que no sean art√≠culos
        if (!url.match(/https?:\/\//)) continue;
        if (url.includes('javascript:')) continue;
        if (url.match(/\/(login|registro|suscripcion|newsletter)/i)) continue;

        // Verificar si ya tenemos esta noticia
        if (articles.some(a => a.url === url)) continue;

        // Buscar imagen cercana
        const linkIndex = html.indexOf(match[0]);
        const htmlContext = html.substring(Math.max(0, linkIndex - 500), linkIndex + 500);
        const imgMatch = htmlContext.match(/<img[^>]+src=["']([^"']+)["']/i);
        const imageUrl = imgMatch ? imgMatch[1] : undefined;

        // Buscar contenido/descripci√≥n cercana
        const paragraphMatch = htmlContext.match(/<p[^>]*>([^<]{50,300})<\/p>/i);
        const content = paragraphMatch
          ? paragraphMatch[1].replace(/<[^>]+>/g, '').trim()
          : title;

        const article: ScrapedArticle = {
          id: crypto.randomUUID(),
          title,
          content,
          url: url.startsWith('http') ? url : new URL(url, sourceUrl).toString(),
          imageUrl: imageUrl?.startsWith('http') ? imageUrl : undefined,
          source: sourceName,
          selected: true,
          scrapedAt: new Date().toISOString(),
        };

        if (validateCategoryArticle(article)) {
          articles.push(article);
        }

        if (articles.length >= 5) break; // M√°ximo 5 noticias
      }
    }

    console.log(`  ‚úÖ ${sourceName}: Extra√≠das ${articles.length} noticias de la p√°gina de categor√≠a`);

    if (articles.length === 0) {
      console.warn(`  ‚ö†Ô∏è  ${sourceName}: No se pudieron extraer noticias`);
      console.warn(`     URL: ${sourceUrl}`);
      console.warn(`     - Markdown preview: ${markdown.substring(0, 300)}`);
    }

    return articles;
  } catch (error) {
    console.error(`  ‚ùå Error parseando categor√≠a de ${sourceName}:`, error);
    return [];
  }
}

/**
 * Valida un art√≠culo de p√°gina de categor√≠a
 * Requisitos m√°s relajados que art√≠culos individuales
 */
function validateCategoryArticle(article: ScrapedArticle): boolean {
  // T√≠tulo m√≠nimo 15 caracteres
  if (!article.title || article.title.length < 15) {
    console.log(`    ‚ö†Ô∏è  Art√≠culo rechazado: t√≠tulo muy corto (${article.title?.length || 0} chars)`);
    return false;
  }

  // URL debe ser v√°lida
  if (!article.url || !article.url.match(/^https?:\/\/.+/)) {
    console.log(`    ‚ö†Ô∏è  Art√≠culo rechazado: URL inv√°lida - "${article.url}"`);
    return false;
  }

  // Filtrar t√≠tulos que no parezcan noticias
  const lowercaseTitle = article.title.toLowerCase();
  const bannedWords = ['publicidad', 'suscr√≠bete', 'newsletter', 'cookie', 'pol√≠tica de privacidad'];
  if (bannedWords.some(word => lowercaseTitle.includes(word))) {
    console.log(`    ‚ö†Ô∏è  Art√≠culo rechazado: t√≠tulo parece ser navegaci√≥n/publicidad`);
    return false;
  }

  return true;
}

/**
 * Parsea la respuesta de Firecrawl y extrae art√≠culos
 *
 * @param response - Respuesta de Firecrawl API
 * @param sourceName - Nombre de la fuente
 * @returns Array de art√≠culos extra√≠dos
 *
 * @example
 * ```ts
 * const response = { success: true, data: { markdown: "..." } };
 * const articles = parseFirecrawlResponse(response, "Primicias");
 * ```
 */
export function parseFirecrawlResponse(
  response: FirecrawlResponse,
  sourceName: string,
  _sourceUrl: string
): ScrapedArticle[] {
  if (!response.data) {
    console.log(`  ‚ö†Ô∏è  ${sourceName}: No data in response`);
    return [];
  }

  const articles: ScrapedArticle[] = [];
  const markdown = response.data.markdown || "";
  const html = response.data.html || "";

  console.log(`  üìä ${sourceName}: markdown=${markdown.length} chars, html=${html.length} chars`);

  try {
    // Estrategia 1: Parsear por headers de markdown (## T√≠tulo)
    const markdownHeaders = markdown.match(/^##\s+(.+?)$([\s\S]+?)(?=^##\s+|$)/gm);

    console.log(`  üîç ${sourceName}: Estrategia 1 (markdown headers) encontr√≥ ${markdownHeaders?.length || 0} matches`);

    if (markdownHeaders && markdownHeaders.length > 0) {
      for (const match of markdownHeaders) {
        const lines = match.split("\n");
        const title = lines[0].replace(/^##\s+/, "").trim();
        const content = lines.slice(1).join("\n").trim();

        // Intentar extraer URL del contenido
        const urlMatch = content.match(/https?:\/\/[^\s)]+/);
        const url = urlMatch ? urlMatch[0] : "";

        const article: ScrapedArticle = {
          id: crypto.randomUUID(),
          title,
          content: content.substring(0, 500), // Limitar a 500 chars
          url,
          source: sourceName,
          selected: true,
          scrapedAt: new Date().toISOString(),
        };

        if (validateArticle(article)) {
          articles.push(article);
        }
      }
    }

    // Estrategia 2: Si no hay headers, intentar con HTML
    if (articles.length === 0 && html) {
      console.log(`  üîç ${sourceName}: Probando Estrategia 2 (HTML parsing)...`);

      // Regex para encontrar t√≠tulos en HTML (h1, h2, h3)
      const htmlTitles = html.match(/<h[123][^>]*>(.+?)<\/h[123]>/gi);

      console.log(`  üîç ${sourceName}: Estrategia 2 encontr√≥ ${htmlTitles?.length || 0} t√≠tulos HTML`);

      if (htmlTitles && htmlTitles.length > 0) {
        for (const titleTag of htmlTitles.slice(0, 20)) {
          // Limitar a 20 art√≠culos
          const title = titleTag.replace(/<[^>]+>/g, "").trim();

          // Buscar contenido despu√©s del t√≠tulo
          const titleIndex = html.indexOf(titleTag);
          const nextTitleIndex = html.indexOf("<h", titleIndex + titleTag.length);
          const endIndex = nextTitleIndex > -1 ? nextTitleIndex : titleIndex + 1000;

          const contentHtml = html.substring(
            titleIndex + titleTag.length,
            endIndex
          );
          const content = contentHtml
            .replace(/<[^>]+>/g, " ")
            .replace(/\s+/g, " ")
            .trim()
            .substring(0, 500);

          // Intentar extraer URL
          const urlMatch = contentHtml.match(/href=["']([^"']+)["']/);
          const url = urlMatch ? urlMatch[1] : "";

          const article: ScrapedArticle = {
            id: crypto.randomUUID(),
            title,
            content,
            url,
            source: sourceName,
            selected: true,
            scrapedAt: new Date().toISOString(),
          };

          if (validateArticle(article)) {
            articles.push(article);
          }
        }
      }
    }

    console.log(`  ‚úÖ ${sourceName}: Parseados ${articles.length} art√≠culos v√°lidos`);

    if (articles.length === 0) {
      console.warn(`  ‚ö†Ô∏è  ${sourceName}: No se pudo extraer ning√∫n art√≠culo v√°lido`);
      console.warn(`     - Markdown preview: ${markdown.substring(0, 200)}`);
      console.warn(`     - HTML preview: ${html.substring(0, 200)}`);
    }

    return articles;
  } catch (error) {
    console.error(`  ‚ùå Error parseando respuesta de ${sourceName}:`, error);
    return [];
  }
}

/**
 * Valida que un art√≠culo tenga los datos m√≠nimos requeridos
 *
 * @param article - Art√≠culo a validar
 * @returns true si el art√≠culo es v√°lido
 *
 * @example
 * ```ts
 * const article = { title: "T√≠tulo", content: "Contenido...", url: "https://...", source: "Primicias" };
 * const isValid = validateArticle(article); // true
 * ```
 */
export function validateArticle(article: ScrapedArticle): boolean {
  // Verificar t√≠tulo min 10 chars
  if (!article.title || article.title.length < 10) {
    console.log(`    ‚ùå Art√≠culo rechazado: t√≠tulo muy corto (${article.title?.length || 0} chars)`);
    return false;
  }

  // Verificar contenido min 50 chars
  if (!article.content || article.content.length < 50) {
    console.log(`    ‚ùå Art√≠culo rechazado: contenido muy corto (${article.content?.length || 0} chars) - "${article.title}"`);
    return false;
  }

  // Verificar que source existe
  if (!article.source) {
    console.log(`    ‚ùå Art√≠culo rechazado: sin source`);
    return false;
  }

  // URL es opcional pero si existe debe ser v√°lida
  if (article.url && !article.url.match(/^https?:\/\/.+/)) {
    console.log(`    ‚ùå Art√≠culo rechazado: URL inv√°lida - "${article.url}"`);
    return false;
  }

  return true;
}
