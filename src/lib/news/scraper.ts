/**
 * M√≥dulo de Scraping de Noticias
 *
 * Arquitectura 100% Crawl4AI:
 * - Extracci√≥n directa de art√≠culos desde p√°ginas de categor√≠a
 * - Opci√≥n de enriquecimiento con contenido completo
 */

import {
  getActiveSources,
  updateSourceLastScraped,
} from "@/lib/db/queries/sources";
import type { NewsSource } from "@/lib/schema";
import { extractCategoryArticles, extractArticles } from "@/lib/crawl4ai";

/**
 * Estructura de un art√≠culo scrapeado
 */
export interface ScrapedArticle {
  id?: string; // UUID generado para identificar la noticia
  title: string;
  content: string; // Excerpt/resumen del art√≠culo
  fullContent?: string; // Contenido completo del art√≠culo
  url: string;
  imageUrl?: string; // URL de la imagen principal
  author?: string; // Autor del art√≠culo
  publishedDate?: string; // Fecha de publicaci√≥n
  source: string;
  selected?: boolean; // Si est√° seleccionada para el bolet√≠n (por defecto true)
  scrapedAt?: string; // Timestamp del scraping
  metadata?: {
    wordCount?: number; // N√∫mero de palabras en el art√≠culo
    readingTime?: number; // Tiempo estimado de lectura (minutos)
    contentQuality?: number; // Score de calidad del contenido (0-100)
  };
}

/**
 * Resultado del scraping
 */
export interface ScrapeResult {
  primicias: ScrapedArticle[];
  laHora: ScrapedArticle[];
  elComercio: ScrapedArticle[];
  teleamazonas: ScrapedArticle[];
  ecu911: ScrapedArticle[];
  metadata: {
    totalArticles: number;
    scrapedAt: string;
    sourcesSuccess: number;
    sourcesFailed: number;
    errors: Array<{ source: string; error: string }>;
  };
}

/**
 * Configuraci√≥n de scraping de una fuente
 */
interface ScrapeConfig {
  urls?: string[];
}

/**
 * Normaliza el nombre de la fuente para usar como key
 *
 * @param name - Nombre de la fuente
 * @returns Key normalizada (camelCase)
 *
 * @example
 * ```ts
 * normalizeSourceName("Primicias") // "primicias"
 * normalizeSourceName("La Hora") // "laHora"
 * normalizeSourceName("El Comercio") // "elComercio"
 * ```
 */
function normalizeSourceName(name: string): keyof Omit<ScrapeResult, "metadata"> {
  const normalized = name
    .toLowerCase()
    .replace(/\s+/g, "")
    .replace(/^el/, "el")
    .replace(/^la/, "la");

  // Mapeo espec√≠fico
  const mapping: Record<string, keyof Omit<ScrapeResult, "metadata">> = {
    primicias: "primicias",
    lahora: "laHora",
    elcomercio: "elComercio",
    teleamazonas: "teleamazonas",
    ecu911: "ecu911",
  };

  return mapping[normalized] || "primicias";
}

/**
 * Scrapea todas las fuentes activas
 *
 * @returns Resultado del scraping con art√≠culos organizados por fuente
 *
 * @example
 * ```ts
 * const result = await scrapeAllSources();
 * console.log(`Total: ${result.metadata.totalArticles} art√≠culos`);
 * console.log(`Primicias: ${result.primicias.length} art√≠culos`);
 * ```
 */
export async function scrapeAllSources(): Promise<ScrapeResult> {
  console.log("üîç Iniciando scraping de todas las fuentes con Crawl4AI...");

  const result: ScrapeResult = {
    primicias: [],
    laHora: [],
    elComercio: [],
    teleamazonas: [],
    ecu911: [],
    metadata: {
      totalArticles: 0,
      scrapedAt: new Date().toISOString(),
      sourcesSuccess: 0,
      sourcesFailed: 0,
      errors: [],
    },
  };

  try {
    // Obtener fuentes activas
    const sources = await getActiveSources();
    console.log(`üì° Encontradas ${sources.length} fuentes activas`);

    // üß™ MODO TEST: Solo scrapear Primicias para diagn√≥stico
    const testMode = true;
    const primiciasSource = sources.find(s => s.name.toLowerCase().includes('primicias'));
    const sourcesToScrape = testMode && primiciasSource ? [primiciasSource] : sources;
    console.log(`üß™ MODO TEST ACTIVADO: Solo scrapeando ${sourcesToScrape.map(s => s.name).join(', ')}`);

    // Scrapear cada fuente
    const scrapePromises = sourcesToScrape.map(async (source) => {
      try {
        console.log(`  ‚Üí Scraping ${source.name}...`);
        const articles = await scrapeSource(source);

        // Actualizar fuente como exitosa
        await updateSourceLastScraped(source.id, "success");

        // Agregar art√≠culos al resultado
        const sourceKey = normalizeSourceName(source.name);
        result[sourceKey] = articles;
        result.metadata.sourcesSuccess++;

        console.log(`  ‚úì ${source.name}: ${articles.length} art√≠culos`);
      } catch (error) {
        console.error(`  ‚úó ${source.name} fall√≥:`, error);

        // Actualizar fuente como fallida
        await updateSourceLastScraped(source.id, "failed");

        result.metadata.sourcesFailed++;
        result.metadata.errors.push({
          source: source.name,
          error: (error as Error).message,
        });
      }
    });

    // Esperar a que todas las fuentes terminen
    await Promise.all(scrapePromises);

    // Calcular total de art√≠culos
    result.metadata.totalArticles =
      result.primicias.length +
      result.laHora.length +
      result.elComercio.length +
      result.teleamazonas.length +
      result.ecu911.length;

    console.log(`‚úÖ Scraping completado: ${result.metadata.totalArticles} art√≠culos totales`);

    return result;
  } catch (error) {
    console.error("‚ùå Error en scrapeAllSources:", error);
    throw new Error(
      `Error scraping fuentes: ${(error as Error).message}`
    );
  }
}

/**
 * Enriquece art√≠culos con contenido completo usando Crawl4AI
 *
 * Esta funci√≥n es OPCIONAL. Despu√©s de extraer art√≠culos de p√°ginas de categor√≠a,
 * puedes usarla para obtener el contenido completo de cada art√≠culo.
 *
 * @param result - Resultado del scraping inicial
 * @param options - Opciones de configuraci√≥n
 * @returns Resultado enriquecido con contenido completo
 *
 * @example
 * ```ts
 * const basicResult = await scrapeAllSources();
 * const enrichedResult = await enrichWithFullContent(basicResult);
 * console.log(`Enriquecidos: ${enrichedResult.metadata.totalArticles} art√≠culos`);
 * ```
 */
export async function enrichWithFullContent(
  result: ScrapeResult,
  options?: {
    maxConcurrency?: number;
    enableEnrichment?: boolean;
  }
): Promise<ScrapeResult> {
  const enableEnrichment = options?.enableEnrichment ?? false;

  if (!enableEnrichment) {
    console.log("‚è≠Ô∏è  Enriquecimiento deshabilitado, saltando...");
    return result;
  }

  console.log("üöÄ Iniciando enriquecimiento con contenido completo...");

  const enrichedResult: ScrapeResult = { ...result };
  let totalEnriched = 0;
  let totalFailed = 0;

  try {
    // Procesar cada fuente
    const sources: Array<keyof Omit<ScrapeResult, "metadata">> = [
      "primicias",
      "laHora",
      "elComercio",
      "teleamazonas",
      "ecu911",
    ];

    // Procesar todas las fuentes EN PARALELO para mayor velocidad
    const enrichPromises = sources.map(async (sourceKey) => {
      const articles = result[sourceKey];

      if (articles.length === 0) {
        console.log(`  ‚è≠Ô∏è  ${sourceKey}: Sin art√≠culos para enriquecer`);
        return { sourceKey, mergedArticles: [], enrichedCount: 0, failedCount: 0 };
      }

      console.log(`  üîç ${sourceKey}: Enriqueciendo ${articles.length} art√≠culos...`);

      try {
        // Extraer URLs de los art√≠culos
        const urls = articles.map((article) => article.url);
        const sourceName = articles[0].source;

        // Usar Crawl4AI para extraer contenido completo
        const fullArticles = await extractArticles(
          urls,
          sourceName,
          options?.maxConcurrency || 5
        );

        // Crear mapa de art√≠culos completos por URL
        const fullArticleMap = new Map(fullArticles.map((article) => [article.url, article]));

        // Contadores locales
        let enrichedCount = 0;
        let failedCount = 0;

        // Merge: Combinar datos b√°sicos con contenido completo
        const mergedArticles = articles.map((article) => {
          const fullArticle = fullArticleMap.get(article.url);

          if (fullArticle) {
            enrichedCount++;
            return {
              ...article,
              fullContent: fullArticle.fullContent,
              author: fullArticle.author || article.author,
              publishedDate: fullArticle.publishedDate || article.publishedDate,
              imageUrl: fullArticle.imageUrl || article.imageUrl,
              metadata: fullArticle.metadata,
            };
          } else {
            failedCount++;
            console.warn(`  ‚ö†Ô∏è  ${sourceKey}: No se pudo enriquecer ${article.url}`);
            return article;
          }
        });

        console.log(
          `  ‚úì ${sourceKey}: ${enrichedCount}/${articles.length} art√≠culos enriquecidos`
        );

        return { sourceKey, mergedArticles, enrichedCount, failedCount };
      } catch (error) {
        console.error(`  ‚úó ${sourceKey} fall√≥ en enriquecimiento:`, error);
        // Mantener art√≠culos originales si falla
        return { sourceKey, mergedArticles: articles, enrichedCount: 0, failedCount: articles.length };
      }
    });

    // Esperar a que todas las fuentes terminen (en paralelo)
    const enrichResults = await Promise.all(enrichPromises);

    // Combinar resultados
    for (const { sourceKey, mergedArticles, enrichedCount, failedCount } of enrichResults) {
      enrichedResult[sourceKey] = mergedArticles;
      totalEnriched += enrichedCount;
      totalFailed += failedCount;
    }

    console.log(
      `‚úÖ Enriquecimiento completado: ${totalEnriched} exitosos, ${totalFailed} fallidos`
    );

    return enrichedResult;
  } catch (error) {
    console.error("‚ùå Error en enrichWithFullContent:", error);
    // En caso de error, retornar resultado original sin enriquecer
    return result;
  }
}

/**
 * Scrapea una fuente individual usando Crawl4AI
 *
 * @param source - Fuente a scrapear
 * @returns Array de art√≠culos scrapeados
 *
 * @throws Error si el scraping falla
 *
 * @example
 * ```ts
 * const source = await getSourceByName("Primicias");
 * const articles = await scrapeSource(source);
 * ```
 */
export async function scrapeSource(source: NewsSource): Promise<ScrapedArticle[]> {
  // Verificar si hay m√∫ltiples URLs en scrapeConfig
  const scrapeConfig = source.scrapeConfig as ScrapeConfig | null;
  const urls = scrapeConfig?.urls || [source.url];

  // üß™ MODO TEST: Solo scrapear la primera URL
  const testMode = true;
  const urlsToScrape = testMode ? urls.slice(0, 1) : urls;
  console.log(`  üìã ${source.name}: ${urlsToScrape.length}/${urls.length} URL(s) a scrapear (MODO TEST)`);

  const allArticles: ScrapedArticle[] = [];

  // Scrapear cada URL con Crawl4AI
  for (let urlIndex = 0; urlIndex < urlsToScrape.length; urlIndex++) {
    const url = urlsToScrape[urlIndex];
    const urlLabel = urlsToScrape.length > 1 ? `${urlIndex + 1}/${urlsToScrape.length}` : "";

    console.log(`  üîó ${source.name} ${urlLabel}: ${url}`);

    try {
      // Usar extractCategoryArticles para p√°ginas de categor√≠a
      const articles = await extractCategoryArticles(url, source.name);

      // Los art√≠culos ya vienen en el formato correcto de ScrapedArticle
      allArticles.push(...articles);
      console.log(`  ‚úì ${source.name} ${urlLabel}: ${articles.length} art√≠culos`);
    } catch (error) {
      console.error(
        `  ‚ö†Ô∏è  ${source.name} ${urlLabel} fall√≥ con Crawl4AI:`,
        (error as Error).message
      );
      // Continuar con la siguiente URL
    }
  }

  console.log(
    `  ‚úÖ ${source.name}: Total ${allArticles.length} art√≠culos de ${urls.length} URL(s)`
  );

  return allArticles;
}
